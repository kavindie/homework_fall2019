{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import sleep\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-11-22 16:15:53,957] Making new env: Ant-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kavindie/Documents/git_repo/gym/gym/envs/mujoco/assets/ant.xml\n",
      "Enter\n"
     ]
    }
   ],
   "source": [
    "#print(envs.registry.all())\n",
    "env = gym.make('Ant-v1')\n",
    "#env = gym.make('Taxi-v2')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter\n",
      "Enter\n",
      "Enter\n",
      "Enter\n",
      "Enter\n",
      "Enter\n",
      "Reward: -3.2025350458327706\n"
     ]
    }
   ],
   "source": [
    "# Let's first do some random steps in the game so you see how the game looks like\n",
    "\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "for _ in range(6):\n",
    "    action = env.action_space.sample() #take step using random action from possible actions (actio_space)\n",
    "    obs, rew, done, info = env.step(action) \n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "    input(\"Enter\")\n",
    "#Print the reward of these random action\n",
    "print(\"Reward: %r\" % rew_tot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "observation_dimention = env.observation_space.shape[0]\n",
    "action_dimention = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "print(observation_dimention)\n",
    "print(action_dimention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out\n",
    "print(env.action_space)\n",
    "NUM_ACTIONS = env.action_space.shape[0]\n",
    "print(NUM_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(111,)\n",
      "\n",
      "Enter\n",
      "Enter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "env.env.s=100 # some random number, you might recognize it\n",
    "env.render()\n",
    "input(\"Enter\")\n",
    "env.env.s = 12 # and some other\n",
    "env.render()\n",
    "input(\"Enter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_ACTIONS 8\n",
      "NUM_STATES 111\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a8ddd7b0800a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mold_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_action_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#choosing an action with the highest future reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;31m# goto the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0ms_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#take the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-a8ddd7b0800a>\u001b[0m in \u001b[0;36mbest_action_value\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0ms_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#take the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrew\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_new\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbest_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Value iteration algorithem\n",
    "NUM_ACTIONS = env.action_space.shape[0]\n",
    "print (\"NUM_ACTIONS\", NUM_ACTIONS)\n",
    "NUM_STATES = env.observation_space.shape[0]\n",
    "print (\"NUM_STATES\", NUM_STATES)\n",
    "V = np.zeros([NUM_STATES]) # The Value for each state\n",
    "Pi = np.zeros([NUM_STATES], dtype=int)  # Our policy with we keep updating to get the optimal policy\n",
    "gamma = 0.9 # discount factor\n",
    "significant_improvement = 0.01\n",
    "\n",
    "def best_action_value(s):\n",
    "    # finds the highest value action (max_a) in state s\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in range (0, NUM_ACTIONS):\n",
    "        env.env.s = s\n",
    "        s_new, rew, done, info = env.step(a) #take the action\n",
    "        v = rew + gamma * V[s_new]\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    return best_a\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    # biggest_change is referred to by the mathematical symbol delta in equations\n",
    "    biggest_change = 0\n",
    "    for s in range (0, NUM_STATES):\n",
    "        old_v = V[s]\n",
    "        action = best_action_value(s) #choosing an action with the highest future reward\n",
    "        env.env.s = s # goto the state\n",
    "        s_new, rew, done, info = env.step(action) #take the action\n",
    "        V[s] = rew + gamma * V[s_new] #Update Value for the state using Bellman equation\n",
    "        Pi[s] = action\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "    iteration += 1\n",
    "    if biggest_change < significant_improvement:\n",
    "        print (iteration,' iterations done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
